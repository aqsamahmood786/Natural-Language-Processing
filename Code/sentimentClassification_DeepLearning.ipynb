{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentimentClassification_DeepLearning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIWuNzqHktPe",
        "outputId": "b9523381-586f-46f9-c348-42d9bb0c1026"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at drive; to attempt to forcibly remount, call drive.mount(\"drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DlOdINik5kT"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "co63IDhlk4kH",
        "outputId": "e912b255-12e2-4e66-ace0-f02d17f7e1bc"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchtext\n",
        "from torchtext.legacy.data import Field, TabularDataset, BucketIterator\n",
        "import re\n",
        "import pandas as pd\n",
        "from sklearn import model_selection\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "pd.set_option('max_colwidth',99999)\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_NXDppql2qq"
      },
      "source": [
        "#Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZfF9AJmltLs"
      },
      "source": [
        "amazon_df = pd .read_csv('/content/drive/My Drive/capstone_project/Data/amazon_cells_labelled.txt', delimiter='\\t', names = ['customer_reviews', 'label'])\n",
        "yelp_df =  pd .read_csv('/content/drive/My Drive/capstone_project/Data/yelp_labelled.txt', delimiter='\\t', names = ['customer_reviews', 'label'])\n",
        "imdb_df =  pd .read_csv('/content/drive/My Drive/capstone_project/Data/imdb_labelled.txt', delimiter='\\t', names = ['customer_reviews', 'label'])\n",
        "final_data = pd.concat([amazon_df,imdb_df,yelp_df])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tmx8EkeLmHKl",
        "outputId": "d24d3503-5bdb-484d-e5a5-ec62889aae2c"
      },
      "source": [
        "#splitting data to train and test sets\n",
        "train, test = model_selection.train_test_split(final_data, test_size=0.3, random_state = 42)\n",
        "train.reset_index(drop = True, inplace = True), test.reset_index(drop = True, inplace = True)\n",
        "train.shape, test.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1923, 2), (825, 2))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DNstpYjmLkD"
      },
      "source": [
        "train.to_csv('/content/drive/My Drive/capstone_project/Data/amazon_train.csv', index = False)\n",
        "test.to_csv('/content/drive/My Drive/capstone_project/Data/amazon_test.csv', index = False)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwiePnzbmT5k"
      },
      "source": [
        "#Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EU9Az2n8mMfs"
      },
      "source": [
        "def clean(text):\n",
        "    \n",
        "    text = re.sub(r'[^A-Za-z0-9]+', ' ', text) \n",
        "    text = re.sub(r'https?:/\\/\\S+', ' ', text) \n",
        "    \n",
        "    return text.strip()\n",
        "def tokenizer(s): \n",
        "    return [w.lower() for w in word_tokenize(clean(s))]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBTKaYR7mjkE"
      },
      "source": [
        "# Create torchtext Tabular Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmr3OIP-mnoC"
      },
      "source": [
        "TEXT = torchtext.legacy.data.Field(tokenize = tokenizer)\n",
        "\n",
        "LABEL = torchtext.legacy.data.LabelField(dtype = torch.float)\n",
        "\n",
        "datafields = [('reviews', TEXT), ('label', LABEL)]\n",
        "\n",
        "trn, tst = torchtext.legacy.data.TabularDataset.splits(path = '/content/drive/My Drive/capstone_project/Data/', \n",
        "                                                train = 'amazon_train.csv',\n",
        "                                                test = 'amazon_test.csv',    \n",
        "                                                format = 'csv',\n",
        "                                                skip_header = True,\n",
        "                                                fields = datafields)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1wGpEuinHjJ"
      },
      "source": [
        "#Loading the data into batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ra0rfit1nGYy"
      },
      "source": [
        "train_iterator, test_iterator = torchtext.legacy.data.BucketIterator.splits(\n",
        "                                (trn, tst),\n",
        "                                batch_size = 64,\n",
        "                                sort_key=lambda x: len(x.reviews),\n",
        "                                sort_within_batch=False)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PFsDwSKnaxc"
      },
      "source": [
        "# Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG6ji_2em4yO"
      },
      "source": [
        "## Load pretrained GloVe word vectors and build vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RopXQuHOmt4Z"
      },
      "source": [
        "TEXT.build_vocab(trn, max_size=25000,\n",
        "                 vectors=\"glove.6B.100d\",\n",
        "                 unk_init=torch.Tensor.normal_)\n",
        "\n",
        "LABEL.build_vocab(trn)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1DUkp2Rn_co"
      },
      "source": [
        "# A two-layer Bidirectional Gated Recurrent Unit Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3g75m8UoICc"
      },
      "source": [
        "class Bi_GRU(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, \n",
        "                 output_dim, n_layers, bidirectional, dropout):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        self.biGru = nn.GRU(embedding_dim, hidden_dim, num_layers = n_layers, \n",
        "                           bidirectional = bidirectional, dropout=dropout)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        \n",
        "    def forward(self, text):\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        \n",
        "        output, hidden = self.biGru(embedded)\n",
        "        \n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
        "       \n",
        "        return self.fc(hidden.squeeze(0))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pCGOL2goVw1"
      },
      "source": [
        "input_dim = len(TEXT.vocab)\n",
        "embedding_dim = 100\n",
        "hidden_dim = 20\n",
        "output_dim = 1\n",
        "n_layers = 2\n",
        "bidirectional = True\n",
        "dropout = 0.5\n",
        "model=Bi_GRU(input_dim,embedding_dim,hidden_dim,output_dim,n_layers,bidirectional,dropout)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgIEayN3xtM1",
        "outputId": "90efbfb2-f6a7-4f45-c2b4-c4f0ab12ecc7"
      },
      "source": [
        "model"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Bi_GRU(\n",
              "  (embedding): Embedding(4232, 100)\n",
              "  (biGru): GRU(100, 20, num_layers=2, dropout=0.5, bidirectional=True)\n",
              "  (fc): Linear(in_features=40, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aruYVTPx2WPN"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "unk_idx = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "pad_idx = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "model.embedding.weight.data[unk_idx] = torch.zeros(embedding_dim)\n",
        "model.embedding.weight.data[pad_idx] = torch.zeros(embedding_dim)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Qg3Y3NHolXL"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZJcqQPKonEO"
      },
      "source": [
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "\n",
        "  model.train()\n",
        "  \n",
        "  for batch in iterator:\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      predictions = model(batch.reviews).squeeze(1)\n",
        "      \n",
        "      loss = criterion(predictions, batch.label)\n",
        "      \n",
        "      rounded_preds = torch.round(torch.sigmoid(predictions))\n",
        "      correct = (rounded_preds == batch.label).float() \n",
        "      \n",
        "      acc = correct.sum() / len(correct)\n",
        "      \n",
        "      loss.backward()\n",
        "      \n",
        "      optimizer.step()\n",
        "      \n",
        "      epoch_loss += loss.item()\n",
        "      epoch_acc += acc.item()\n",
        "\n",
        "      \n",
        "  return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66TPMCveosjc",
        "outputId": "efb42ce7-beee-46a9-e9eb-f408838fe033"
      },
      "source": [
        "num_epochs = 50\n",
        "loss = []\n",
        "acc = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  train_loss, train_acc= train(model, train_iterator, optimizer, criterion)#\n",
        "  print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% |')\n",
        "  acc.append(train_acc)\n",
        "  loss.append(train_loss)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch: 01 | Train Loss: 0.697 | Train Acc: 48.62% |\n",
            "| Epoch: 02 | Train Loss: 0.684 | Train Acc: 55.59% |\n",
            "| Epoch: 03 | Train Loss: 0.682 | Train Acc: 56.38% |\n",
            "| Epoch: 04 | Train Loss: 0.665 | Train Acc: 60.62% |\n",
            "| Epoch: 05 | Train Loss: 0.631 | Train Acc: 65.51% |\n",
            "| Epoch: 06 | Train Loss: 0.548 | Train Acc: 74.24% |\n",
            "| Epoch: 07 | Train Loss: 0.466 | Train Acc: 80.39% |\n",
            "| Epoch: 08 | Train Loss: 0.436 | Train Acc: 81.13% |\n",
            "| Epoch: 09 | Train Loss: 0.364 | Train Acc: 84.76% |\n",
            "| Epoch: 10 | Train Loss: 0.376 | Train Acc: 85.42% |\n",
            "| Epoch: 11 | Train Loss: 0.301 | Train Acc: 88.51% |\n",
            "| Epoch: 12 | Train Loss: 0.339 | Train Acc: 86.98% |\n",
            "| Epoch: 13 | Train Loss: 0.340 | Train Acc: 87.03% |\n",
            "| Epoch: 14 | Train Loss: 0.248 | Train Acc: 90.98% |\n",
            "| Epoch: 15 | Train Loss: 0.259 | Train Acc: 91.57% |\n",
            "| Epoch: 16 | Train Loss: 0.211 | Train Acc: 92.89% |\n",
            "| Epoch: 17 | Train Loss: 0.207 | Train Acc: 93.15% |\n",
            "| Epoch: 18 | Train Loss: 0.193 | Train Acc: 93.20% |\n",
            "| Epoch: 19 | Train Loss: 0.208 | Train Acc: 92.83% |\n",
            "| Epoch: 20 | Train Loss: 0.164 | Train Acc: 94.30% |\n",
            "| Epoch: 21 | Train Loss: 0.148 | Train Acc: 94.96% |\n",
            "| Epoch: 22 | Train Loss: 0.147 | Train Acc: 94.96% |\n",
            "| Epoch: 23 | Train Loss: 0.147 | Train Acc: 95.46% |\n",
            "| Epoch: 24 | Train Loss: 0.137 | Train Acc: 95.67% |\n",
            "| Epoch: 25 | Train Loss: 0.190 | Train Acc: 93.48% |\n",
            "| Epoch: 26 | Train Loss: 0.166 | Train Acc: 94.99% |\n",
            "| Epoch: 27 | Train Loss: 0.111 | Train Acc: 96.67% |\n",
            "| Epoch: 28 | Train Loss: 0.119 | Train Acc: 96.62% |\n",
            "| Epoch: 29 | Train Loss: 0.109 | Train Acc: 96.47% |\n",
            "| Epoch: 30 | Train Loss: 0.098 | Train Acc: 96.77% |\n",
            "| Epoch: 31 | Train Loss: 0.115 | Train Acc: 96.67% |\n",
            "| Epoch: 32 | Train Loss: 0.105 | Train Acc: 96.35% |\n",
            "| Epoch: 33 | Train Loss: 0.104 | Train Acc: 97.13% |\n",
            "| Epoch: 34 | Train Loss: 0.083 | Train Acc: 97.68% |\n",
            "| Epoch: 35 | Train Loss: 0.081 | Train Acc: 97.18% |\n",
            "| Epoch: 36 | Train Loss: 0.079 | Train Acc: 97.63% |\n",
            "| Epoch: 37 | Train Loss: 0.081 | Train Acc: 97.38% |\n",
            "| Epoch: 38 | Train Loss: 0.073 | Train Acc: 97.83% |\n",
            "| Epoch: 39 | Train Loss: 0.076 | Train Acc: 97.53% |\n",
            "| Epoch: 40 | Train Loss: 0.077 | Train Acc: 97.88% |\n",
            "| Epoch: 41 | Train Loss: 0.066 | Train Acc: 98.08% |\n",
            "| Epoch: 42 | Train Loss: 0.068 | Train Acc: 97.93% |\n",
            "| Epoch: 43 | Train Loss: 0.060 | Train Acc: 97.98% |\n",
            "| Epoch: 44 | Train Loss: 0.068 | Train Acc: 98.19% |\n",
            "| Epoch: 45 | Train Loss: 0.060 | Train Acc: 97.98% |\n",
            "| Epoch: 46 | Train Loss: 0.043 | Train Acc: 98.84% |\n",
            "| Epoch: 47 | Train Loss: 0.053 | Train Acc: 98.59% |\n",
            "| Epoch: 48 | Train Loss: 0.064 | Train Acc: 97.88% |\n",
            "| Epoch: 49 | Train Loss: 0.060 | Train Acc: 98.39% |\n",
            "| Epoch: 50 | Train Loss: 0.055 | Train Acc: 98.34% |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcwksPVZ7SCd"
      },
      "source": [
        "## Test the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZR6TXfksq9_",
        "outputId": "0f30a0fd-73e6-409c-db28-26f4f62351c5"
      },
      "source": [
        "epoch_loss = 0\n",
        "epoch_acc = 0\n",
        "from sklearn.metrics import confusion_matrix\n",
        "nb_classes = 2\n",
        "\n",
        "confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  for batch in test_iterator:\n",
        "    predictions = model(batch.reviews).squeeze(1)\n",
        "\n",
        "    loss = criterion(predictions, batch.label)\n",
        "    classes = batch.label\n",
        "    rounded_preds = torch.round(torch.sigmoid(predictions))\n",
        "    correct = (rounded_preds == batch.label).float() \n",
        "    a = correct.tolist()\n",
        "\n",
        "    acc = correct.sum()/len(correct)\n",
        "\n",
        "    epoch_loss += loss.item()\n",
        "    epoch_acc += acc.item()\n",
        "    for t, p in zip(classes.view(-1), rounded_preds.view(-1)):\n",
        "      confusion_matrix[t.long(), p.long()] += 1\n",
        "\n",
        "\n",
        "test_loss = epoch_loss / len(test_iterator)\n",
        "test_acc = epoch_acc / len(test_iterator)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.569 | Test Acc: 85.25%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2u2eE24668Pw",
        "outputId": "928e487a-49cb-4f2b-d971-4cd99c67d7b6"
      },
      "source": [
        "print(confusion_matrix)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[340.,  50.],\n",
            "        [ 72., 363.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Voh6ExjT7XfT"
      },
      "source": [
        "# Save model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRktf3gS7QMZ"
      },
      "source": [
        "torch.save(model,'/content/drive/My Drive/capstone_project/Data/rnn.h5')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhOTq0aY7eAR"
      },
      "source": [
        "##loading the model\n",
        "#loaded_model = torch.load('/content/drive/My Drive/capstone_project/Data/rnn.h5')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}